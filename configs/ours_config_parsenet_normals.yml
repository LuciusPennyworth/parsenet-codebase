[train]
model_name = "bs_{}"
log_dir = "logs/0311_run"

log_interval = 10
relation_loss_weight = 0.1
ms_iter = 3

# Dataset path
#dataset = "/home/xiaoqin/Code/abc-data/"
dataset = "/home/zhuhan/Code/ProjectMarch/dataset/hpnet"
train_list = "train_data.txt"
test_list = "test_data.txt"
augment = 0
if_normal_noise = 0
train_skip = 4
val_skip = 50
batch_size = 8 # set 2 when DEBUG, set 16 when TRAIN(8 gpus)

# Use Positional Encoding
position_encode = True
encode_level = 5 # L in NeRF

# pre-trained models
# Whether to load a pretrained models or not,
# if the "preload_model" term is "True", pretrain model will load from
# os.path.join(pretrain_model_path, pretrain_model_name)
preload_model = True
pretrain_model_path = "/home/zhuhan/Code/relationCNN/logs/pretrained_models/" # ex. /home/zhuhan/Code/relationCNN/logs/trained_models/debug_voteloss
pretrain_model_name = "new_abc_normal.tar"

# Whether to input the normals or not
normals = True
proportion = 1.0

# number of training instance
num_train=24000
num_val=4000
num_test=4000  # set 10 to run my mini-test dataset, in order to run the whole test dataset, 1.set num_test to 4000.  2. switch file path in "dataset.py::line 83"
num_points=8000
loss_weight=100

num_epochs = 80
grid_size = 20

# batch size, based on the GPU memory

# Optimization
optim = adam

# Epsilon for the RL training, not applicable in Supervised training
accum = 4

weight_decay = 0.0  # l2 Weight decay
# dropout for Decoder network
dropout = 0.2
# Learing rate
lr = 0.01
# Encoder dropout
encoder_drop = 0.2
# Whether to schedule the learning rate or not
lr_sch = True
# Number of epochs to wait before decaying the learning rate.
patience = 8

mode = 5
